# -*- coding: utf-8 -*-
"""tarefa4-sprint8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CtwPwBlHiAOk6BISe5XFfbntrmUSoyDu
"""

pip install --upgrade pyspark

from google.colab import drive
drive.mount('/content/drive', force_remount=True)
!cp /content/drive/MyDrive/archive_stores/*.* /content

from pyspark.sql import SparkSession
from pyspark import SparkContext, SQLContext
from pyspark.sql.functions import lit, expr
from pyspark.sql.functions import count
from pyspark.sql.functions import col
import pyspark.sql.functions as F
import random

# Criando uma SparkSession
spark = SparkSession.builder \
    .master('local[*]') \
    .appName('Exercicio Intro') \
    .getOrCreate()

"""###Nesta etapa, adicione código para ler o arquivo nomes_aleatorios.txt através do comando spark.read.csv. Carregue-o para dentro de um dataframe chamado df_nomes e, por fim, liste algumas linhas através do método show. Exemplo: df_nomes.show(5)"""

df_nomes = spark.read.csv('/content/drive/MyDrive/análise de dados/nomes_aleatorios.txt')
df_nomes.show(5)

"""###Como não informamos no momento da leitura do arquivo, o Spark não identificou o Schema por padrão e definiu todas as colunas como string. Para ver o Schema, use o método df_nomes.printSchema(). Nesta etapa, será necessário adicionar código para renomear a coluna para Nomes, imprimir o esquema e mostrar 10 linhas do dataframe."""

df_nomes.printSchema()

df_nomes = df_nomes.withColumnRenamed("_c0", "Nomes")
df_nomes.printSchema()

df_nomes.show(5)

def add_coluna_randomizada(nome_coluna,lista,dataframe):
  return dataframe.withColumn(
      nome_coluna,
      F.array(
          [F.lit(x) for x in lista]
      ).getItem(
          (F.rand()*len(lista)).cast("int")
      )
  )

"""### Ao dataframe (df_nomes), adicione nova coluna chamada Escolaridade e atribua para cada linha um dos três valores de forma aleatória: Fundamental, Medio ou Superior."""

escolaridade = ['Fundamental','Medio','Superior']

df_nomes = add_coluna_randomizada("Escolaridade",escolaridade,df_nomes)

df_nomes.show(10)

"""###Ao dataframe (df_nomes), adicione nova coluna chamada Pais e atribua para cada linha o nome de um dos 13 países da América do Sul, de forma aleatória."""

paises = ["Peru","Bolivia","Argentina","Brasil","Chile","Colômbia","Equador",
          "Guiana","Paraguai","Suriname","Uruguai","Venezuela"]

df_nomes = add_coluna_randomizada("Pais",paises,df_nomes)

df_nomes.show(5)

"""### Ao dataframe (df_nomes), adicione nova coluna chamada AnoNascimento e atribua para cada linha um valor de ano entre 1945 e 2010, de forma aleatória."""

anos = range(1945,2010)

df_nomes = add_coluna_randomizada("AnoNascimento",anos,df_nomes)

df_nomes.show(5)

"""### Usando o método select do dataframe (df_nomes), selecione as pessoas que nasceram neste século. Armazene o resultado em outro dataframe chamado df_select e mostre 10 nomes deste."""

# Convertendo a coluna "AnoNascimento" para tipo inteiro
df_nomes = df_nomes.withColumn("AnoNascimento", df_nomes["AnoNascimento"])

# Filtrando os registros de pessoas que nasceram neste século e armazenando o resultado em um novo dataframe
df_select = df_nomes.select("Nomes","AnoNascimento")
df_select = df_select.filter(df_select.AnoNascimento >= 2000)

df_select.show(10)

"""### Usando Spark SQL repita o processo da Pergunta 6."""

# Criando uma tabela temporária chamada 'pessoas' para execução das consultas com Spark SQL
df_nomes.createOrReplaceTempView('pessoas')

# Executando consulta SQL para selecionas as pessoas que nasceram neste século
nascimento_filtrado_sql = spark.sql('SELECT * FROM pessoas WHERE AnoNascimento >= 2000')

nascimento_filtrado_sql.show(10)

"""### Usando o método select do Dataframe df_nomes, Conte o número de pessoas que são da geração Millennials (nascidos entre 1980 e 1994) no Dataset"""

# Filtrando as pessoas que nasceram entre 1980 e 1994
df_millennials = df_nomes.filter((col('AnoNascimento') >= 1980) & (col('AnoNascimento') <= 1994))

# Contando o número de pessoas da geração Millennials
count_millennials = df_millennials.count()

print(f"Número de pessoas da geração Millennials: {count_millennials}")

"""### Repita o processo da Pergunta 8 utilizando Spark SQL"""

# Executando consulta SQL para contar o número de pessoas da geração Millennials
resultado_sql = spark.sql("SELECT COUNT(*) AS count_millennials FROM pessoas WHERE AnoNascimento BETWEEN 1980 AND 1994")

# Extraindo o resultado da consulta SQL
count_millennials_sql = resultado_sql.first()["count_millennials"]

print(f"Número de pessoas da geração Millennials: {count_millennials}")

# Criando uma função para gerar a consulta SQL de contagem por geração
def consulta_geracao(geracao,ano_inicial,ano_final):
  query = f"""
      SELECT Pais, '{geracao}' AS Geracao, COUNT(*) AS Quantidade
      FROM pessoas
      WHERE AnoNascimento BETWEEN {ano_inicial} AND {ano_final}
      GROUP BY Pais
  """
  return query

# Informações de cada geração
geracoes = [
    {"nome": "Baby Boomers", "ano_inicio": 1944, "ano_fim": 1964},
    {"nome": "Geração X", "ano_inicio": 1965, "ano_fim": 1979},
    {"nome": "Millennials", "ano_inicio": 1980, "ano_fim": 1994},
    {"nome": "Geração Z", "ano_inicio": 1995, "ano_fim": 2015}
]

# Consultando os resultados para cada geração e armazenando em uma lista
resultados_geracoes = []
for geracao_info in geracoes:
    geracao_query = consulta_geracao(geracao_info["nome"],geracao_info["ano_inicio"],geracao_info["ano_fim"])
    resultado = spark.sql(geracao_query)
    resultados_geracoes.append(resultado)

# Combinando os DataFrames usando union
df_resultado = resultados_geracoes[0]
for i in range(1, len(resultados_geracoes)):
    df_resultado = df_resultado.union(resultados_geracoes[i])

# Ordenando o resultado por Pais, Geração e Quantidade
df_resultado = df_resultado.orderBy("Pais", "Geracao", "Quantidade")

df_resultado.show()